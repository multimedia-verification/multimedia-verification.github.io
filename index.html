<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Grand Challenge on Multimedia Verification">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ACMMM25-Grand Challenge on Multimedia Verification</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: left;">
      <a class="navbar-item" href="https://multimedia-verification.github.io/index.html">
        <span class="icon">
            <i class="fas fa-home"></i>
        </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Challenge iteration
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://detecting-cheapfakes.github.io/icmr-2024.html">
            ACM ICMR 2024
          </a>
          <!-- Add previous iteration here -->
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ACMMM25 - Grand Challenge on Multimedia Verification</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www4.uib.no/en/find-employees/Duc.Tien.Dang.Nguyen">Duc-Tien Dang-Nguyen</a><sup>1</sup>,</span>
            <span class="author-block">
              <a>Morten Langfeldt Dahlback</a><sup>2</sup>,</span>
            <span class="author-block">
              <a>Minh-Son Dao</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://pakkunandy.github.io/">Anh-Duy Tran</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://sohailahmedkhan.github.io/">Sohail Khan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a>Kha-Luan Pham</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a >Michael Riegler</a><sup>6</sup>,
            </span>
            <span class="author-block">
              <a >MPål Halvorsen</a><sup>7</sup>,
            </span>
            <span class="author-block">
              <a>Marc Gallofre Ocana</a><sup>2,7</sup>,
            </span>
            <span class="author-block">
              <a>Henrik B. Vold</a><sup>8</sup>,
            </span>
            <span class="author-block">
              <a>Silje Førsund</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a>Minh-Triet Tran</a><sup>9</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Bergen</span>
            <span class="author-block"><sup>2</sup>Faktisk.no</span>
            <span class="author-block"><sup>3</sup>NICT</span>
            <span class="author-block"><sup>4</sup>KU Leuven</span>
            <span class="author-block"><sup>5</sup>Aalto University</span>
            <span class="author-block"><sup>6</sup>Simula Research Laboratory</span>
            <span class="author-block"><sup>7</sup>Vimond</span>
            <span class="author-block"><sup>8</sup>Institutt for Journalistikk</span>
            <span class="author-block"><sup>9</sup>University of Science - VNUHCM</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://forms.gle/pEo3LoF9edZoUKXq6"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa-solid fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/multimedia-verification/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa-brands fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://groups.google.com/g/multimedia-verification"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa-solid fa-users-line"></i>
                  </span>
                  <span>Google group</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/intro.png">
      <span>Deepfakes (left): These are falsified media created using sophisticated AI-based media manipulation tools and techniques. Cheapfakes (right): These include falsified media created with/without contemporary non-AI based editing tools which are easily accessible. Photoshopping tools can be used to tamper with images. Videos can be sped up or slowed down to change the intent or misrepresent the person in the video. Re-contextualizing includes associating falsified or unrelated claims with a genuine image to misrepresent events or persons. This challenge is focused on detecting re-contextualized cheapfakes.</span>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>The <b>Grand Challenge on Multimedia Verification</b> is an open competition where researchers and practitioners compete to verify the authenticity and context of multimedia content, addressing real-world misinformation challenges. Participants analyze provided images and videos, assess sources, detect manipulations, and submit a detailed verification report.</p>
          <p>In addition to the main task, participants can also take part in the <b>OOC Subtask (Out-of-Context Detection)</b>, which focuses on identifying whether multimedia content has been misused in an incorrect or misleading context. This subtask continues the work from the Grand Challenge on Detecting Cheapfakes, previously held at ACM MMSys 2021, ACM MM 2022, IEEE ICME 2023, and ACM ICMR 2024.</p>
          <p>There are no restrictions on methodology: solutions can be fully manual, fully automated, or a mix of OSINT, existing tools, and newly developed techniques.</p>
          <p>Participants can choose to:</p>
          <ul>
            <li>Compete in the main task.</li>
            <li>Compete in the OOC Subtask only.</li>
            <li>Compete in both the main task and the OOC Subtask.</li>
          </ul>
          <p>Additionally, as part of the main task, participants may provide a concise verification summary for general readers. This component is optional and will be evaluated separately based on the main task results.</p>
          <p>The competition consists of three stages: <b>Training</b>, <b>Validation</b>, and <b>Real-World Verification</b>. Participants first explore 100 known cases to familiarize themselves with the tasks. In the Validation stage, they must analyze 20 new cases and <i>pass an evaluation to qualify for the Real-World Verification Stage</i>, where 10 live verification cases will test their methods on fresh, evolving misinformation challenges.</p>
          <p><b>Recognition & Awards.</b> Teams submitting a scientific paper may present at ACM Multimedia, subject to peer review. Only paper-submitting teams are eligible for awards, but all participants’ scores will be officially recognized.</p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop is-centered">
    <div class="content">
      <h2 class="title is-3">Main Task</h2>
      <h3 class="title is-4">Task Description </h3>
      <p><strong>Objective:</strong> Verify the authenticity and context of multimedia content, <strong>which may be in a language other than English</strong>, and provide both a detailed report for fact-checkers and a concise summary for general readers. The languages of the content might not be in English.</p>

      <p><strong>Input:</strong> Each case/task will provide (in a zip file):</p>
      <ul>
          <li><strong>Multimedia Content:</strong> Image(s) or video(s)</li>
          <li><strong>Associated Context:</strong> Captions, descriptions, social media posts, news articles, metadata (if available)</li>
          <li><strong>Additional Clues:</strong> Possible sources, claims, or fact-checker notes (if relevant)</li>
      </ul>
      
      <p><strong>Output:</strong> A verification report (as a text file) <strong>in English</strong>. The report should include the following key information:</p>
      
      <ul>
          <li><strong>Summary of Key Points:</strong> Provide a concise overview of the content, including relevant details. Clearly highlight any uncertainties and underline what is not yet known.</li>
          <li><strong>Content Category:</strong> Assign relevant tags based on platforms, people, brands, or specific topics.</li>
          <li><strong>Forensic Analysis Results:</strong>
              <ul>
                  <li><strong>Authenticity:</strong> Determine if the content is synthetic, modified, or recaptured.</li>
                  <li><strong>Tools &amp; Methods Used:</strong> Specify the verification tools and techniques applied.</li>
                  <li><strong>Synthetic Type (if applicable):</strong> Identify whether it was generated using GANs, Stable Diffusion, or other AI models.</li>
                  <li><strong>Other Artifacts:</strong> Note any detected anomalies or manipulations.</li>
              </ul>
          </li>
          <li><strong>Verified Evidence:</strong> State what can be confirmed about the video/photo based on available evidence.
              <ul>
                  <li><strong>Source Details:</strong> Where the content comes from (URLs, original posts, etc.)</li>
                  <li><strong>Where?</strong> (Location)</li>
                  <li><strong>When?</strong> (Time)</li>
                  <li><strong>Who?</strong> (People, organizations, entities involved)</li>
                  <li><strong>Why?</strong> (Possible motivations or intent)</li>
              </ul>
          </li>
          <li><strong>Other Evidence &amp; Findings:</strong> Any additional relevant information, supporting materials, or external sources</li>
      </ul>
      
      <p><strong>Note:</strong> For each evidence finding, specify the failure type if verification fails: <em>Indeterminate</em> (insufficient or ambiguous data), <em>Inconclusive</em> (attempted but no definitive result), or <em>Not Feasible</em> (limited expertise or tools).</p>
      
      <h3 class="title is-4">Competition Stages </h3>
      
      <ul>
          <li><strong>Stage 1 - Training (March - April)</strong>
              <ul>
                  <li>Organizers provide 100 known cases with input and expected outputs.</li>
                  <li>Participants register, explore tasks, and practice verification.</li>
              </ul>
          </li>
      
          <li><strong>Stage 2 - Validation (May)</strong>
              <ul>
                  <li>Organizers release 20 new cases with input only.</li>
                  <li>Participants must submit verification reports and describe their methodology. Participants may also submit a scientific paper detailing their approach.</li>
                  <li>Submissions are evaluated, and <strong>only those who successfully pass the validation stage will qualify for the final competition</strong>.</li>
              </ul>
          </li>
      
          <li><strong>Stage 3 - Real-World Verification (July 28 - August 8)</strong>
              <ul>
                  <li>Only validated participants advance to this stage.</li>
                  <li>Organizers provide 10 real-time cases, reflecting ongoing misinformation challenges.</li>
                  <li>Participants verify cases, submit results, and optionally submit a camera-ready paper.</li>
              </ul>
          </li>
      </ul>
      <h3 class="title is-4">Evaluation Criteria</h3>

      <h4 class="title is-5">Overall Evaluation</h4>
      Main score on each task:
      <pre>Score = max(0, Q - w.T²)</pre>
      <ul>
          <li><strong>Q:</strong> Quality score assessed by professional fact-checkers (see details in Section <a href="#qualityevaluation">Quality Evaluation</a>).</li>
          <li><strong>T:</strong> Time - number of hours after the task release.</li>
          <li><strong>w:</strong> Weight - controls the penalty. (<em>w = 0.001</em>)</li>
          <li><strong>Simply put, the score decreases proportionally to the square of the time spent.</strong></li>
      </ul>
      
      <h4 class="title is-5">Report Evaluation (Quality Q)</h4>
      <a name="qualityevaluation"></a>
      <ol>
          <li><strong>Summary & Content Classification (10%)</strong>
              <ul>
                  <li><strong>Concise Overview</strong> – Clearly summarizes the findings, highlighting uncertainties and unknowns.</li>
                  <li><strong>Correctly Categorized</strong> – Correctly assign relevant tags based on platforms, people, brands, or specific topics (e.g., TikTok, Trump, Coca-Cola, Ukraine War, AI-generated).</li>
              </ul>
          </li>
      
          <li><strong>Verified Evidence (50%)</strong>
              <ul>
                  <li><strong>Source Details</strong> – Identifies where the content originates (URLs, original posts, metadata).</li>
                  <li><strong>Where? (Location)</strong> – Determines the correct geographical context.</li>
                  <li><strong>When? (Time)</strong> – Establishes the accurate timeframe.</li>
                  <li><strong>Who? (People, Organizations, Entities Involved)</strong> – Identifies key individuals or groups.</li>
                  <li><strong>Why? (Motivation or Intent)</strong> – Provides a reasoned explanation of possible intent.</li>
              </ul>
          </li>
      
          <li><strong>Forensic Analysis (15%)</strong>
              <ul>
                  <li><strong>Authenticity Assessment</strong> – Determines if the content is synthetic, modified, or recaptured.</li>
                  <li><strong>Verification Tools & Methods</strong> – Clearly documents the tools and techniques used.</li>
                  <li><strong>Synthetic Type (if applicable)</strong> – Identifies AI-generated content (e.g., GANs, Stable Diffusion).</li>
                  <li><strong>Other Artifacts</strong> – Notes any detected anomalies or manipulations.</li>
              </ul>
          </li>
      
          <li><strong>Other Evidence & Findings (15%)</strong>
              <ul>
                  <li><strong>Supporting Sources</strong> – Uses additional fact-checks, reports, or metadata to back claims.</li>
                  <li><strong>Cross-Checking Information</strong> – Ensures verification through multiple independent sources.</li>
              </ul>
          </li>
      
          <li><strong>Clarity & Structure (10%)</strong>
              <ul>
                  <li><strong>Well-Organized Report</strong> – Logically structured for readability.</li>
                  <li><strong>Concise & Understandable Language</strong> – Avoids unnecessary complexity or ambiguity.</li>
              </ul>
          </li>
      </ol>
      
      <strong>Note:</strong> Not all points may be verifiable in every case. Clearly stating the failure type (<em>"indeterminate"</em>, <em>"inconclusive"</em>, or <em>"not feasible"</em>) is a valid verification outcome and should be included where necessary.
      
      <h4 class="title is-5">Verification Summarization</h4>
      The summarization subtask will be evaluated by a jury committee based on the following criteria:
      <ol>
          <li><strong>Clarity:</strong> The summary should be well-structured, easy to read, and clearly convey the verification findings.</li>
          <li><strong>Conciseness:</strong> The summary should be brief while still covering the essential points of the verification.</li>
          <li><strong>Readability:</strong> The language should be accessible to a general audience, avoiding technical jargon.</li>
          <li><strong>Accuracy:</strong> The summary should correctly reflect the key findings from the detailed verification report.</li>
      </ol>
      
      The jury committee will assess submissions to ensure they effectively communicate the verification findings in a clear, concise, and accessible manner.
    </div>
  </div>
</section>
  
<section class="section">
  <div class="container is-max-desktop is-centered">
    <div class="content">
      <h2 class="title is-3">Challenge Tasks</h2>
      <p>An image serves as evidence of an event described by a news caption. Presenting an image as evidence of untrue and/or unrelated events is defined as out-of-context (OOC) use of the image. The aim of this challenge is to develop and benchmark models that can be used to detect OOC misuse of images in news items.</p>

      <h3 class="title is-4">Task 1: Detection of Conflicting Image-Caption Triplets </h3>
      <p>If two captions associated with an image are valid, then they should describe the same event. If they refer to same object(s) in the image, but are semantically different, i.e., associate the same subject to different events, this indicates OOC use of the image. However, if the captions correspond to the same event, irrespective of the object(s) they describe, this is defined as not-out-of-context (NOOC) use of the image. </p>
      <p>In this task, participants are asked to come up with methods to detect conflicting image-caption triplets, which indicate miscontextualization. More specifically, given (Image,Caption1,Caption2) triplets as input, their proposed model should predict corresponding class labels 1 (OOC) or 0 (NOOC). The goal is not to identify individual captions as true/false, but rather to detect the existence of miscontextualization. Such methods are considered particularly useful for assisting fact checkers, as highlighting conflicting image-caption triplets allows them to narrow down their search space.</p>

      <h3 class="title is-4">Task 2: Detection of Fake Captions</h3>
      <p>In a practical scenario, multiple captions might not be available for a given image, and the challenge boils down to figuring out whether an individual caption associated with an image is genuine or not.</p>
      <p>In this task, participants are asked to come up with methods to determine whether a given image-caption pair is genuine (real) or falsely generated (fake). More specifically, given an <Image,Caption> pair as input, their proposed model should predict corresponding class labels 0 (real) or 1 (fake).</p>
      <p>We acknowledge that this is a challenging task without prior knowledge of the image origin, even for human moderators. In fact, Luo et al [1] have verified this challenge with a study on human evaluators, who were instructed not to use search engines, where the average human accuracy was only around 65%.</p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="content">
      <h2 class="title is-3">Evaluation Criteria</h2>
      <p>Participant models will be evaluated and ranked according to two aggregate scores, composed of 5 and 3 metrics respectively.</p>
      <ul>
        <li><strong>Effectiveness</strong>: accuracy, precision, recall, F1-score, and Matthews correlation coefficient (MCC). Participants are asked to calculate these 5 metrics for their model and include the values in the results section of their submission.</li>
        <li><strong>Efficiency</strong>: latency, number of parameters, and model size. Participants are asked to calculate these 3 metrics for their model and include the values in the results section of their submission.</li>
      </ul>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop is-centered">
    <div class="content">
      <h2 class="title is-3">Important dates</h2>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop is-centered">
    <div class="content">
      <h2 class="title is-3">Submission Guidelines</h2>
      
      <h3 class="title is-4">Docker Container</h3>
      <p>We recommend challenge participants to submit their solutions as a Docker container, since it will make sure that we don't get any errors resulting from software incompatibility issues or any other similar reason. In this case, we recommend them to follow the instructions given under <a href="https//github.com/detecting-cheapfakes/detecting-cheapfakes-code">here</a>.</p>      
    
      <h3 class="title is-4">Standard Python Executable</h3>
      <p>If the participants face any difficulties in submitting their solutions as a Docker container, or if they feel more comfortable submitting their solution as a standard Python project, they can do so by following the instructions below. It would also be helpful for us if the participants use PyTorch as the main library if they would like to submit their Python projects, however, this is not compulsory.</p>      
      <ul>
        <li>We expect that the submitted code will be executable by a single command, for example:
          <p><code>python solution.py &lt;path to folder containing the hidden test split file
          private_test.json&gt;</code></p>
        </li>

        <li>Participants should expect the same format for both the <strong>private_test.json</strong> file and the <strong>public_test.json</strong> file.</li>

        <li>To cope with any software incompatibility issues, we request the participants to provide a <code>requirements.txt</code> file along with their solutions, containing the names and specific version numbers of software packages used. This is fairly easy to do with both <strong>Conda</strong>(<code>conda list</code>), and <strong>Pip</strong>(<code>pip list</code>). We recommend that the participants use Conda and create a fresh environment before starting to write the code for their challenge solution.</li>
      </ul>

      <h3 class="title is-4">Jupyter Notebook</h3>
      <p>Participants can also submit their solutions using Jupyter notebooks, by following the instructions below.</p>
      <ul>
        <li>
          Participants should structure their code so that it allows us to change the input path by updating a single line in the main file, i.e., the submitted notebook should be executable after changing the INPUT_FOLDER parameter from the path for the public test split file, to the path for the private test split file, as shown below:
          <p><code>INPUT_FOLDER = &lt;path to folder containing the hidden test split file private_test.json&gt;</code></p>
        </li>

        <li>Participants should expect the same format for both the <strong>private_test.json</strong> file and the <strong>public_test.json</strong> file.</li>
      </ul>
    </div>
  </div>
</section>

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->

<footer class="footer">
  <div class="container">
    <div class="content has-text-left">
      <p>Borrowed from <a href="https://github.com/nerfies/nerfies.github.io">source code</a></p>
    </div>
  </div>
</footer>

</body>
</html>
