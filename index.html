<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Grand Challenge on Multimedia Verification">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ACMMM25-Grand Challenge on Multimedia Verification</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: left;">
      <a class="navbar-item" href="https://multimedia-verification.github.io/index.html">
        <span class="icon">
            <i class="fas fa-home"></i>
        </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Challenge iteration
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://detecting-cheapfakes.github.io/icmr-2024.html">
            ACM ICMR 2024
          </a>
          <!-- Add previous iteration here -->
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ACMMM25 - Grand Challenge on Multimedia Verification</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www4.uib.no/en/find-employees/Duc.Tien.Dang.Nguyen">Duc-Tien Dang-Nguyen</a><sup>1</sup>,</span>
            <span class="author-block">
              <a>Morten Langfeldt Dahlback</a><sup>2</sup>,</span>
            <span class="author-block">
              <a>Minh-Son Dao</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://pakkunandy.github.io/">Anh-Duy Tran</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://sohailahmedkhan.github.io/">Sohail Khan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a>Kha-Luan Pham</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a >Michael Riegler</a><sup>6</sup>,
            </span>
            <span class="author-block">
              <a >MPål Halvorsen</a><sup>7</sup>,
            </span>
            <span class="author-block">
              <a>Marc Gallofre Ocana</a><sup>2,7</sup>,
            </span>
            <span class="author-block">
              <a>Henrik B. Vold</a><sup>8</sup>,
            </span>
            <span class="author-block">
              <a>Silje Førsund</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a>Minh-Triet Tran</a><sup>9</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Bergen, Norway</span>
            <span class="author-block"><sup>2</sup>Faktisk.no, Norway</span>
            <span class="author-block"><sup>3</sup>NICT, Japan</span>
            <span class="author-block"><sup>4</sup>KU Leuven, Belgium</span>
            <span class="author-block"><sup>5</sup>Aalto University, Finland</span>
            <span class="author-block"><sup>6</sup>Simula Research Laboratory, Norway</span>
            <span class="author-block"><sup>7</sup>Vimond, Norway</span>
            <span class="author-block"><sup>8</sup>Institutt for Journalistikk, Norway</span>
            <span class="author-block"><sup>9</sup>University of Science - VNUHCM, Vietnam</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://forms.gle/pEo3LoF9edZoUKXq6"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa-solid fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/detecting-cheapfakes/detecting-cheapfakes-code"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa-brands fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://groups.google.com/g/grandchallenge-cheapfakes"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa-solid fa-users-line"></i>
                  </span>
                  <span>Google group</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/intro.png">
      <span>Deepfakes (left): These are falsified media created using sophisticated AI-based media manipulation tools and techniques. Cheapfakes (right): These include falsified media created with/without contemporary non-AI based editing tools which are easily accessible. Photoshopping tools can be used to tamper with images. Videos can be sped up or slowed down to change the intent or misrepresent the person in the video. Re-contextualizing includes associating falsified or unrelated claims with a genuine image to misrepresent events or persons. This challenge is focused on detecting re-contextualized cheapfakes.</span>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop is-centered">
    <div class="content">
      <h2 class="title is-3">Challenge Tasks</h2>
      <p>An image serves as evidence of an event described by a news caption. Presenting an image as evidence of untrue and/or unrelated events is defined as out-of-context (OOC) use of the image. The aim of this challenge is to develop and benchmark models that can be used to detect OOC misuse of images in news items.</p>

      <h3 class="title is-4">Task 1: Detection of Conflicting Image-Caption Triplets </h3>
      <p>If two captions associated with an image are valid, then they should describe the same event. If they refer to same object(s) in the image, but are semantically different, i.e., associate the same subject to different events, this indicates OOC use of the image. However, if the captions correspond to the same event, irrespective of the object(s) they describe, this is defined as not-out-of-context (NOOC) use of the image. </p>
      <p>In this task, participants are asked to come up with methods to detect conflicting image-caption triplets, which indicate miscontextualization. More specifically, given (Image,Caption1,Caption2) triplets as input, their proposed model should predict corresponding class labels 1 (OOC) or 0 (NOOC). The goal is not to identify individual captions as true/false, but rather to detect the existence of miscontextualization. Such methods are considered particularly useful for assisting fact checkers, as highlighting conflicting image-caption triplets allows them to narrow down their search space.</p>

      <h3 class="title is-4">Task 2: Detection of Fake Captions</h3>
      <p>In a practical scenario, multiple captions might not be available for a given image, and the challenge boils down to figuring out whether an individual caption associated with an image is genuine or not.</p>
      <p>In this task, participants are asked to come up with methods to determine whether a given image-caption pair is genuine (real) or falsely generated (fake). More specifically, given an <Image,Caption> pair as input, their proposed model should predict corresponding class labels 0 (real) or 1 (fake).</p>
      <p>We acknowledge that this is a challenging task without prior knowledge of the image origin, even for human moderators. In fact, Luo et al [1] have verified this challenge with a study on human evaluators, who were instructed not to use search engines, where the average human accuracy was only around 65%.</p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="content">
      <h2 class="title is-3">Evaluation Criteria</h2>
      <p>Participant models will be evaluated and ranked according to two aggregate scores, composed of 5 and 3 metrics respectively.</p>
      <ul>
        <li><strong>Effectiveness</strong>: accuracy, precision, recall, F1-score, and Matthews correlation coefficient (MCC). Participants are asked to calculate these 5 metrics for their model and include the values in the results section of their submission.</li>
        <li><strong>Efficiency</strong>: latency, number of parameters, and model size. Participants are asked to calculate these 3 metrics for their model and include the values in the results section of their submission.</li>
      </ul>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop is-centered">
    <div class="content">
      <h2 class="title is-3">Important dates</h2>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop is-centered">
    <div class="content">
      <h2 class="title is-3">Submission Guidelines</h2>
      
      <h3 class="title is-4">Docker Container</h3>
      <p>We recommend challenge participants to submit their solutions as a Docker container, since it will make sure that we don't get any errors resulting from software incompatibility issues or any other similar reason. In this case, we recommend them to follow the instructions given under <a href="https//github.com/detecting-cheapfakes/detecting-cheapfakes-code">here</a>.</p>      
    
      <h3 class="title is-4">Standard Python Executable</h3>
      <p>If the participants face any difficulties in submitting their solutions as a Docker container, or if they feel more comfortable submitting their solution as a standard Python project, they can do so by following the instructions below. It would also be helpful for us if the participants use PyTorch as the main library if they would like to submit their Python projects, however, this is not compulsory.</p>      
      <ul>
        <li>We expect that the submitted code will be executable by a single command, for example:
          <p><code>python solution.py &lt;path to folder containing the hidden test split file
          private_test.json&gt;</code></p>
        </li>

        <li>Participants should expect the same format for both the <strong>private_test.json</strong> file and the <strong>public_test.json</strong> file.</li>

        <li>To cope with any software incompatibility issues, we request the participants to provide a <code>requirements.txt</code> file along with their solutions, containing the names and specific version numbers of software packages used. This is fairly easy to do with both <strong>Conda</strong>(<code>conda list</code>), and <strong>Pip</strong>(<code>pip list</code>). We recommend that the participants use Conda and create a fresh environment before starting to write the code for their challenge solution.</li>
      </ul>

      <h3 class="title is-4">Jupyter Notebook</h3>
      <p>Participants can also submit their solutions using Jupyter notebooks, by following the instructions below.</p>
      <ul>
        <li>
          Participants should structure their code so that it allows us to change the input path by updating a single line in the main file, i.e., the submitted notebook should be executable after changing the INPUT_FOLDER parameter from the path for the public test split file, to the path for the private test split file, as shown below:
          <p><code>INPUT_FOLDER = &lt;path to folder containing the hidden test split file private_test.json&gt;</code></p>
        </li>

        <li>Participants should expect the same format for both the <strong>private_test.json</strong> file and the <strong>public_test.json</strong> file.</li>
      </ul>
    </div>
  </div>
</section>

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->

<footer class="footer">
  <div class="container">
    <div class="content has-text-left">
      <p>Borrowed from <a href="https://github.com/nerfies/nerfies.github.io">source code</a></p>
    </div>
  </div>
</footer>

</body>
</html>
